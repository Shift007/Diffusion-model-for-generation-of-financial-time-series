{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pywt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from diffusion.unet import UNet\n",
    "from diffusion.diffusion import Diffusion\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.signal import find_peaks\n",
    "# Import detrending pipeline\n",
    "from wavelet_detrending import WaveletDetrendingPipeline, visualize_detrending_effect\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115caae8",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Cosine-Based Time Series\n",
    "\n",
    "Create time series by combining multiple cosine waves with:\n",
    "- Random frequencies (1-10 Hz)\n",
    "- Random amplitudes (0.3-2.0)\n",
    "- Random phase shifts\n",
    "- Gaussian noise\n",
    "- Linear trend component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3304e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic cosine-based time series with realistic properties\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Parameters\n",
    "num_synthetic_series = 50000  # Number of synthetic time series to generate\n",
    "series_length = 64  # Same as window_size for real data\n",
    "num_frequencies = 5  # Number of cosine components to combine\n",
    "\n",
    "# Storage for synthetic series\n",
    "synthetic_volume_series = []\n",
    "\n",
    "for i in range(num_synthetic_series):\n",
    "    # Initialize time series\n",
    "    t = np.linspace(0, 1, series_length)\n",
    "    ts = np.zeros(series_length)\n",
    "    \n",
    "    # Combine multiple cosine waves with random parameters\n",
    "    for _ in range(num_frequencies):\n",
    "        # Random frequency (higher frequencies create more variation)\n",
    "        freq = np.random.uniform(1, 10)\n",
    "        \n",
    "        # Random amplitude (creates volume clustering effect)\n",
    "        amplitude = np.random.uniform(0.3, 2.0)\n",
    "        \n",
    "        # Random phase shift\n",
    "        phase = np.random.uniform(0, 2 * np.pi)\n",
    "        \n",
    "        # Add cosine component\n",
    "        ts += amplitude * np.cos(2 * np.pi * freq * t + phase)\n",
    "    \n",
    "    # Add Gaussian noise for realism (mimics market microstructure noise)\n",
    "    noise_level = 0.3\n",
    "    ts += np.random.randn(series_length) * noise_level\n",
    "    \n",
    "    # Add trend component (mimics volume trends)\n",
    "    trend = np.random.uniform(-0.5, 0.5) * t\n",
    "    ts += trend\n",
    "    \n",
    "    synthetic_volume_series.append(ts)\n",
    "\n",
    "synthetic_volume_series = np.array(synthetic_volume_series)\n",
    "\n",
    "print(f\"Generated {num_synthetic_series} synthetic time series\")\n",
    "print(f\"Shape: {synthetic_volume_series.shape}\")\n",
    "print(f\"Raw statistics BEFORE normalization:\")\n",
    "print(f\"  Mean: {synthetic_volume_series.mean():.4f}\")\n",
    "print(f\"  Std: {synthetic_volume_series.std():.4f}\")\n",
    "print(f\"  Min: {synthetic_volume_series.min():.4f}\")\n",
    "print(f\"  Max: {synthetic_volume_series.max():.4f}\")\n",
    "\n",
    "# Visualize 3 random synthetic series\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "random_indices = random.sample(range(num_synthetic_series), 3)\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    ts = synthetic_volume_series[idx]\n",
    "    axes[i].plot(ts, color='tab:orange', linewidth=1.5)\n",
    "    axes[i].set_ylabel('Synthetic Volume', color='tab:orange')\n",
    "    axes[i].set_title(f'Synthetic Cosine-based Time Series #{idx}')\n",
    "    axes[i].set_xlabel('Day')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "fig.suptitle('Generated Synthetic Time Series (Before Normalization)', fontsize=14, y=1.0)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf9f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Z-Score normalization (same as real GOOG data preprocessing)\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"GLOBAL Z-SCORE NORMALIZATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Compute global statistics\n",
    "synthetic_mean = float(synthetic_volume_series.mean())\n",
    "synthetic_std = float(synthetic_volume_series.std())\n",
    "\n",
    "# Apply global z-score normalization\n",
    "synthetic_normalized = (synthetic_volume_series - synthetic_mean) / synthetic_std\n",
    "\n",
    "print(f\"\\nOriginal synthetic data statistics:\")\n",
    "print(f\"  Mean: {synthetic_mean:.4f}\")\n",
    "print(f\"  Std: {synthetic_std:.4f}\")\n",
    "print(f\"  Min: {synthetic_volume_series.min():.4f}\")\n",
    "print(f\"  Max: {synthetic_volume_series.max():.4f}\")\n",
    "\n",
    "print(f\"\\nNormalized data statistics:\")\n",
    "print(f\"  Mean: {float(synthetic_normalized.mean()):.6f}\")\n",
    "print(f\"  Std: {float(synthetic_normalized.std()):.6f}\")\n",
    "print(f\"  Min: {synthetic_normalized.min():.4f}\")\n",
    "print(f\"  Max: {synthetic_normalized.max():.4f}\")\n",
    "\n",
    "# Visualize distributions before and after normalization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Before normalization\n",
    "axes[0].hist(synthetic_volume_series.flatten(), bins=100, color='steelblue', edgecolor='black', alpha=0.7, density=True)\n",
    "axes[0].set_title('Synthetic Data - Before Normalization', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Value', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# After normalization\n",
    "axes[1].hist(synthetic_normalized.flatten(), bins=100, color='coral', edgecolor='black', alpha=0.7, density=True)\n",
    "axes[1].set_title('Synthetic Data - After Global Z-Score', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Normalized Value', fontsize=12)\n",
    "axes[1].set_ylabel('Density', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Normalization parameters saved for inverse transformation:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Synthetic: mean={synthetic_mean:.6f}, std={synthetic_std:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3927192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation analysis for synthetic data\n",
    "\n",
    "max_lag = 60\n",
    "\n",
    "# Flatten synthetic series for autocorrelation\n",
    "synthetic_flat = synthetic_normalized.flatten()\n",
    "\n",
    "# Calculate ACF\n",
    "synthetic_acf = acf(synthetic_flat, nlags=max_lag, fft=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.stem(range(len(synthetic_acf)), synthetic_acf, linefmt='orange', markerfmt='o')\n",
    "plt.title('Autocorrelation: Synthetic Cosine Volume', fontsize=14)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('ACF')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-0.3, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Autocorrelation Values:\")\n",
    "print(f\"  ACF(1): {synthetic_acf[1]:.4f}\")\n",
    "print(f\"  ACF(5): {synthetic_acf[5]:.4f}\")\n",
    "print(f\"  ACF(10): {synthetic_acf[10]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d8d198",
   "metadata": {},
   "source": [
    "## 4. Generate Wavelet Images\n",
    "\n",
    "Transform normalized time series to wavelet images using SWT (Stationary Wavelet Transform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate wavelet images from synthetic cosine time series using DETRENDING\n",
    "# This should prevent mode collapse by removing mean bias\n",
    "\n",
    "wavelet = 'haar'\n",
    "level = int(np.log2(64))  # Max level for length 64\n",
    "\n",
    "# Initialize detrending pipeline\n",
    "synthetic_wavelet_pipeline = WaveletDetrendingPipeline(wavelet=wavelet, level=level)\n",
    "\n",
    "# Preprocess with detrending\n",
    "synthetic_wavelet_images, synth_detrending_stats = synthetic_wavelet_pipeline.preprocess(\n",
    "    synthetic_normalized, \n",
    "    return_stats=True\n",
    ")\n",
    "\n",
    "print(\"=== SYNTHETIC WAVELET PREPROCESSING WITH DETRENDING ===\")\n",
    "print(f\"Synthetic wavelet image array shape: {synthetic_wavelet_images.shape}\")\n",
    "print(f\"\\nDetrending Statistics:\")\n",
    "print(f\"  Original data mean: {synth_detrending_stats['original_mean']:.4f}\")\n",
    "print(f\"  Original data std: {synth_detrending_stats['original_std']:.4f}\")\n",
    "print(f\"  Detrended data mean: {synth_detrending_stats['detrended_mean']:.6f}\")\n",
    "print(f\"  Detrended data std: {synth_detrending_stats['detrended_std']:.4f}\")\n",
    "print(\"\\nWavelet coefficients (before normalization):\")\n",
    "print(f\"  Mean: {synth_detrending_stats['wavelet_mean_before_norm']:.4f}\")\n",
    "print(f\"  Std: {synth_detrending_stats['wavelet_std_before_norm']:.4f}\")\n",
    "print(\"\\nRobust normalization using 5th-95th percentiles:\")\n",
    "print(f\"  p5: {synth_detrending_stats['p5']:.4f}\")\n",
    "print(f\"  p95: {synth_detrending_stats['p95']:.4f}\")\n",
    "print(\"\\nFinal normalized wavelet images:\")\n",
    "print(f\"  Range: [{synthetic_wavelet_images.min():.4f}, {synthetic_wavelet_images.max():.4f}]\")\n",
    "print(f\"  Mean: {synth_detrending_stats['final_mean']:.4f}\")\n",
    "print(f\"  Std: {synth_detrending_stats['final_std']:.4f}\")\n",
    "\n",
    "# Visualize detrending effect\n",
    "print(\"\\n=== VISUALIZING DETRENDING EFFECT ===\")\n",
    "visualize_detrending_effect(synthetic_normalized, synthetic_wavelet_pipeline, idx=100)\n",
    "\n",
    "# Visualize synthetic wavelet images\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i in range(3):\n",
    "    idx = random_indices[i] if i < len(random_indices) else i\n",
    "    if idx < len(synthetic_wavelet_images):\n",
    "        im = axes[i].imshow(synthetic_wavelet_images[idx, :, :, 0], aspect='auto', origin='lower', cmap='viridis')\n",
    "        axes[i].set_title(f'Synthetic Wavelet Image #{idx} (DETRENDED)')\n",
    "        axes[i].set_ylabel('Frequency Level')\n",
    "        axes[i].set_xlabel('Time')\n",
    "        plt.colorbar(im, ax=axes[i], label='Normalized Coefficients')\n",
    "\n",
    "plt.suptitle('SWT Haar Wavelet Images - DETRENDED (Synthetic Cosine Data)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save synthetic wavelet images\n",
    "np.save('synthetic_cosine_wavelet_images.npy', synthetic_wavelet_images)\n",
    "np.save('synthetic_cosine_time_series.npy', synthetic_normalized)\n",
    "print(f\"\\nSaved files:\")\n",
    "print(f\"  - synthetic_cosine_wavelet_images.npy\")\n",
    "print(f\"  - synthetic_cosine_time_series.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24aa25",
   "metadata": {},
   "source": [
    "## 5. Train Diffusion Model on Synthetic Data\n",
    "\n",
    "Train the diffusion model using same architecture and hyperparameters as real data experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train diffusion model on synthetic cosine wavelet images\n",
    "# Same architecture and hyperparameters as real data training\n",
    "\n",
    "OUT_DIR_SYNTH = './diffusion_checkpoints_synthetic'\n",
    "N_SAMPLES_FOR_VIS = 8\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "TIMESTEPS = 1000\n",
    "SAVE_EVERY_STEPS = 20000\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "os.makedirs(OUT_DIR_SYNTH, exist_ok=True)\n",
    "\n",
    "# Use synthetic wavelet images\n",
    "imgs_synth = synthetic_wavelet_images\n",
    "\n",
    "# Ensure values in [0,1]\n",
    "if imgs_synth.dtype != np.float32:\n",
    "    imgs_synth = imgs_synth.astype('float32')\n",
    "\n",
    "if imgs_synth.ndim != 4 or imgs_synth.shape[-1] != 1:\n",
    "    raise RuntimeError(f\"Unexpected shape: {imgs_synth.shape}. Expected (N, H, W, 1)\")\n",
    "\n",
    "H_synth, W_synth = imgs_synth.shape[1], imgs_synth.shape[2]\n",
    "\n",
    "class WaveletDataset(Dataset):\n",
    "    def __init__(self, arr):\n",
    "        self.arr = arr\n",
    "    def __len__(self):\n",
    "        return len(self.arr)\n",
    "    def __getitem__(self, i):\n",
    "        img = self.arr[i]\n",
    "        img = np.transpose(img, (2, 0, 1)).copy()\n",
    "        img = img * 2.0 - 1.0\n",
    "        return torch.from_numpy(img)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset_synth = WaveletDataset(imgs_synth)\n",
    "loader_synth = DataLoader(dataset_synth, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Model and diffusion\n",
    "model_synth = UNet(in_channels=1).to(DEVICE)\n",
    "diffusion_synth = Diffusion(model_synth, timesteps=TIMESTEPS, device=DEVICE, beta_schedule='cosine')\n",
    "opt_synth = torch.optim.AdamW(model_synth.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "# Helper: sample and save grid\n",
    "@torch.no_grad()\n",
    "def sample_and_save_synth(step, num_samples=N_SAMPLES_FOR_VIS):\n",
    "    model_synth.eval()\n",
    "    try:\n",
    "        samples = diffusion_synth.sample((num_samples, 1, H_synth, W_synth))\n",
    "    except Exception as e:\n",
    "        print('Sampling failed:', e)\n",
    "        torch.cuda.empty_cache()\n",
    "        samples = diffusion_synth.sample((min(num_samples, 4), 1, H_synth, W_synth))\n",
    "    samples = samples.clamp(-1, 1)\n",
    "    samples = (samples + 1.0) / 2.0\n",
    "    grid = vutils.make_grid(samples.cpu(), nrow=min(8, num_samples))\n",
    "    out_path = os.path.join(OUT_DIR_SYNTH, f'samples_step_{step}.png')\n",
    "    vutils.save_image(grid, out_path)\n",
    "    print('Saved samples to', out_path)\n",
    "    model_synth.train()\n",
    "\n",
    "# Training loop\n",
    "model_synth.train()\n",
    "global_step_synth = 0\n",
    "\n",
    "print(f\"\\nTraining on SYNTHETIC cosine data...\")\n",
    "print(f\"Dataset size: {len(dataset_synth)} samples\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(loader_synth, desc=f'[SYNTHETIC] Epoch {epoch+1}/{EPOCHS}')\n",
    "    for batch in pbar:\n",
    "        imgs_batch = batch.to(DEVICE)\n",
    "        bs = imgs_batch.shape[0]\n",
    "        \n",
    "        t = torch.randint(0, diffusion_synth.timesteps, (bs,), device=DEVICE).long()\n",
    "        loss = diffusion_synth.p_losses(imgs_batch, t, loss_type='huber')\n",
    "\n",
    "        opt_synth.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_synth.parameters(), 1.0)\n",
    "        opt_synth.step()\n",
    "\n",
    "        global_step_synth += 1\n",
    "        pbar.set_postfix({'loss': float(loss.detach().cpu().item()), 'step': global_step_synth})\n",
    "\n",
    "        if global_step_synth % SAVE_EVERY_STEPS == 0:\n",
    "            ckpt = {'model': model_synth.state_dict(), 'opt': opt_synth.state_dict(), \n",
    "                    'epoch': epoch, 'step': global_step_synth}\n",
    "            ckpt_path = os.path.join(OUT_DIR_SYNTH, f'ckpt_step_{global_step_synth}.pt')\n",
    "            torch.save(ckpt, ckpt_path)\n",
    "            print(f'Saved checkpoint to {ckpt_path}')\n",
    "            sample_and_save_synth(global_step_synth)\n",
    "\n",
    "    # End of epoch checkpoint\n",
    "    ckpt = {'model': model_synth.state_dict(), 'opt': opt_synth.state_dict(), \n",
    "            'epoch': epoch + 1, 'step': global_step_synth}\n",
    "    torch.save(ckpt, os.path.join(OUT_DIR_SYNTH, f'ckpt_epoch_{epoch+1}.pt'))\n",
    "    print(f'Saved epoch {epoch+1} checkpoint')\n",
    "\n",
    "print('Training on synthetic data finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9dfca5",
   "metadata": {},
   "source": [
    "## 6. Generate Samples from Trained Model\n",
    "\n",
    "Use the trained model to generate new synthetic wavelet images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new samples from trained synthetic model\n",
    "# Compare generated vs original synthetic data\n",
    "\n",
    "CHECKPOINT_DIR_SYNTH = './diffusion_checkpoints_synthetic'\n",
    "NUM_SAMPLES_SYNTH = 1000\n",
    "TEMPERATURE = 1.5\n",
    "\n",
    "# Find latest checkpoint\n",
    "checkpoint_files_synth = glob.glob(os.path.join(CHECKPOINT_DIR_SYNTH, 'ckpt_epoch_*.pt'))\n",
    "if not checkpoint_files_synth:\n",
    "    raise RuntimeError(f\"No checkpoint files found in {CHECKPOINT_DIR_SYNTH}\")\n",
    "\n",
    "latest_checkpoint_synth = max(checkpoint_files_synth, key=os.path.getctime)\n",
    "print(f\"Loading synthetic model checkpoint: {latest_checkpoint_synth}\")\n",
    "\n",
    "# Load model\n",
    "model_gen_synth = UNet(in_channels=1).to(DEVICE)\n",
    "diffusion_gen_synth = Diffusion(model_gen_synth, timesteps=1000, device=DEVICE, beta_schedule='cosine')\n",
    "\n",
    "checkpoint_synth = torch.load(latest_checkpoint_synth, map_location=DEVICE)\n",
    "model_gen_synth.load_state_dict(checkpoint_synth['model'])\n",
    "model_gen_synth.eval()\n",
    "\n",
    "print(f\"Generating {NUM_SAMPLES_SYNTH} samples from synthetic-trained model with temperature={TEMPERATURE}...\")\n",
    "\n",
    "generated_from_synth = diffusion_gen_synth.sample((NUM_SAMPLES_SYNTH, 1, H_synth, W_synth), temperature=TEMPERATURE)\n",
    "\n",
    "# Convert from [-1,1] to [0,1]\n",
    "generated_from_synth = generated_from_synth.clamp(-1, 1)\n",
    "generated_from_synth = (generated_from_synth + 1.0) / 2.0\n",
    "\n",
    "# Save\n",
    "generated_from_synth_np = generated_from_synth.cpu().numpy()\n",
    "generated_from_synth_np = np.transpose(generated_from_synth_np, (0, 2, 3, 1))\n",
    "np.save('generated_from_synthetic_model.npy', generated_from_synth_np)\n",
    "print(f\"Saved generated samples to 'generated_from_synthetic_model.npy'\")\n",
    "\n",
    "# Visualize grid\n",
    "grid = vutils.make_grid(generated_from_synth.cpu(), nrow=4, padding=2, normalize=False)\n",
    "grid_np = grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.imshow(grid_np[:, :, 0], cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Normalized Wavelet Coefficients')\n",
    "plt.title(f'Generated Samples from Synthetic-Trained Model ({NUM_SAMPLES_SYNTH} samples)', fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display individual samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(8, NUM_SAMPLES_SYNTH)):\n",
    "    sample = generated_from_synth[i, 0].cpu().numpy()\n",
    "    axes[i].imshow(sample, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[i].set_title(f'Generated Sample {i+1}')\n",
    "    axes[i].set_xlabel('Time')\n",
    "    axes[i].set_ylabel('Frequency Level')\n",
    "\n",
    "for i in range(min(8, NUM_SAMPLES_SYNTH), 8):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {NUM_SAMPLES_SYNTH} samples successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6adc6d",
   "metadata": {},
   "source": [
    "## 7. Reconstruct Time Series from Generated Wavelet Images\n",
    "\n",
    "Apply inverse wavelet transform to convert back to time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct time series from generated wavelet images using DETRENDING PIPELINE\n",
    "# This will restore the DC component for realistic variance\n",
    "\n",
    "# Load generated samples\n",
    "try:\n",
    "    generated_imgs = generated_from_synth.cpu().numpy()\n",
    "    generated_imgs = np.transpose(generated_imgs, (0, 2, 3, 1))\n",
    "except NameError:\n",
    "    if os.path.exists('generated_from_synthetic_model.npy'):\n",
    "        generated_imgs = np.load('generated_from_synthetic_model.npy')\n",
    "        print(\"Loaded generated images from file\")\n",
    "    else:\n",
    "        raise RuntimeError(\"No generated images available\")\n",
    "\n",
    "print(f\"Generated wavelet images shape: {generated_imgs.shape}\")\n",
    "\n",
    "# Use detrending pipeline for reconstruction with DC restoration\n",
    "if 'synthetic_wavelet_pipeline' not in globals():\n",
    "    raise RuntimeError(\"synthetic_wavelet_pipeline not found. Run the wavelet preprocessing cell first.\")\n",
    "\n",
    "print(\"\\n=== RECONSTRUCTION WITH DETRENDING PIPELINE ===\")\n",
    "\n",
    "# Reconstruct with automatic DC restoration\n",
    "reconstructed_generated = synthetic_wavelet_pipeline.postprocess(generated_imgs, original_length=64)\n",
    "\n",
    "print(f\"Reconstructed shape: {reconstructed_generated.shape}\")\n",
    "print(f\"Reconstruction complete with DC component restored!\")\n",
    "\n",
    "# Show detrending statistics\n",
    "detrending_info = synthetic_wavelet_pipeline.get_detrending_info()\n",
    "print(f\"\\n=== DETRENDING STATISTICS ===\")\n",
    "print(f\"DC component (mean) statistics:\")\n",
    "print(f\"  Mean of means: {detrending_info['mean_of_means']:.6f}\")\n",
    "print(f\"  Std of means: {detrending_info['std_of_means']:.6f}\")\n",
    "print(f\"  Range: [{detrending_info['min_mean']:.6f}, {detrending_info['max_mean']:.6f}]\")\n",
    "\n",
    "# Visualize reconstructed series\n",
    "num_to_vis = min(8, len(reconstructed_generated))\n",
    "fig, axes = plt.subplots(4, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_to_vis):\n",
    "    vol_ts = reconstructed_generated[i]\n",
    "    ax = axes[i]\n",
    "    \n",
    "    ax.plot(vol_ts, color='tab:purple', linewidth=1.5)\n",
    "    ax.set_ylabel('Reconstructed Volume', color='tab:purple')\n",
    "    ax.set_title(f'Generated Time Series #{i+1} (DC Restored)')\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i in range(num_to_vis, 8):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Reconstructed Time Series from Generated Wavelet Images (with DC Restoration)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save reconstructed series\n",
    "np.save('reconstructed_generated_synthetic.npy', reconstructed_generated)\n",
    "print(f\"\\nSaved reconstructed time series to 'reconstructed_generated_synthetic.npy'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24e6d5",
   "metadata": {},
   "source": [
    "## 8. Autocorrelation Analysis: Original vs Generated\n",
    "\n",
    "Compare temporal structure between original synthetic data and model-generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef32328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation comparison: Original Synthetic vs Generated\n",
    "# Analyze if the model learned the temporal structure\n",
    "\n",
    "max_lag = 60\n",
    "\n",
    "# Original synthetic data (input to model)\n",
    "original_synth_flat = synthetic_normalized.flatten()\n",
    "\n",
    "# Generated data (output from model)\n",
    "generated_flat = reconstructed_generated.flatten()\n",
    "\n",
    "# Calculate ACFs\n",
    "original_synth_acf = acf(original_synth_flat, nlags=max_lag, fft=True)\n",
    "generated_acf = acf(generated_flat, nlags=max_lag, fft=True)\n",
    "\n",
    "# Side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].stem(range(len(original_synth_acf)), original_synth_acf, linefmt='orange', markerfmt='o')\n",
    "axes[0].set_title('Original Synthetic Cosine - Autocorrelation')\n",
    "axes[0].set_xlabel('Lag')\n",
    "axes[0].set_ylabel('ACF')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(-0.3, 1.0)\n",
    "\n",
    "axes[1].stem(range(len(generated_acf)), generated_acf, linefmt='purple', markerfmt='o')\n",
    "axes[1].set_title('Generated from Model - Autocorrelation')\n",
    "axes[1].set_xlabel('Lag')\n",
    "axes[1].set_ylabel('ACF')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(-0.3, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overlay comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(original_synth_acf)), original_synth_acf, 'o-', color='orange', \n",
    "         label='Original Synthetic', alpha=0.7, markersize=4)\n",
    "plt.plot(range(len(generated_acf)), generated_acf, 's-', color='purple', \n",
    "         label='Generated', alpha=0.7, markersize=4)\n",
    "plt.title('Autocorrelation Comparison: Original Synthetic vs Generated')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('ACF')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-0.3, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical comparison\n",
    "print(f\"\\n=== Autocorrelation Comparison ===\")\n",
    "print(f\"Original Synthetic ACF(1): {original_synth_acf[1]:.4f}\")\n",
    "print(f\"Generated ACF(1): {generated_acf[1]:.4f}\")\n",
    "print(f\"Difference: {abs(original_synth_acf[1] - generated_acf[1]):.4f}\")\n",
    "print(f\"\\nOriginal Synthetic ACF(5): {original_synth_acf[5]:.4f}\")\n",
    "print(f\"Generated ACF(5): {generated_acf[5]:.4f}\")\n",
    "print(f\"Difference: {abs(original_synth_acf[5] - generated_acf[5]):.4f}\")\n",
    "print(f\"\\nOriginal Synthetic ACF(10): {original_synth_acf[10]:.4f}\")\n",
    "print(f\"Generated ACF(10): {generated_acf[10]:.4f}\")\n",
    "print(f\"Difference: {abs(original_synth_acf[10] - generated_acf[10]):.4f}\")\n",
    "\n",
    "# ACF similarity metrics\n",
    "acf_correlation = np.corrcoef(original_synth_acf, generated_acf)[0, 1]\n",
    "acf_mae = np.mean(np.abs(original_synth_acf - generated_acf))\n",
    "acf_rmse = np.sqrt(np.mean((original_synth_acf - generated_acf)**2))\n",
    "\n",
    "print(f\"\\n=== ACF Similarity Metrics ===\")\n",
    "print(f\"ACF Correlation: {acf_correlation:.4f}\")\n",
    "print(f\"ACF MAE: {acf_mae:.4f}\")\n",
    "print(f\"ACF RMSE: {acf_rmse:.4f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Correlation close to 1.0 indicates model captured temporal dependencies\")\n",
    "print(f\"  - Lower MAE/RMSE indicates better pattern matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbe598",
   "metadata": {},
   "source": [
    "## 9. Distribution Analysis: Original vs Generated\n",
    "\n",
    "Comprehensive statistical comparison of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f5213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution comparison: Original Synthetic vs Generated\n",
    "# Comprehensive statistical analysis\n",
    "\n",
    "print(\"=== DISTRIBUTION ANALYSIS ===\")\n",
    "\n",
    "# Original synthetic data\n",
    "original_synth_flat = synthetic_normalized.flatten()\n",
    "\n",
    "# Generated data\n",
    "generated_flat = reconstructed_generated.flatten()\n",
    "\n",
    "print(f\"\\nOriginal Synthetic Statistics:\")\n",
    "print(f\"  Mean: {original_synth_flat.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(original_synth_flat):.4f}\")\n",
    "print(f\"  Std: {original_synth_flat.std():.4f}\")\n",
    "print(f\"  Min: {original_synth_flat.min():.4f}\")\n",
    "print(f\"  Max: {original_synth_flat.max():.4f}\")\n",
    "\n",
    "print(f\"\\nGenerated Statistics:\")\n",
    "print(f\"  Mean: {generated_flat.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(generated_flat):.4f}\")\n",
    "print(f\"  Std: {generated_flat.std():.4f}\")\n",
    "print(f\"  Min: {generated_flat.min():.4f}\")\n",
    "print(f\"  Max: {generated_flat.max():.4f}\")\n",
    "\n",
    "# Statistical tests\n",
    "ks_stat, ks_p_value = ks_2samp(original_synth_flat, generated_flat)\n",
    "print(f\"\\n=== STATISTICAL TESTS ===\")\n",
    "print(f\"Kolmogorov-Smirnov Test:\")\n",
    "print(f\"  Statistic: {ks_stat:.4f}\")\n",
    "print(f\"  P-value: {ks_p_value:.4f}\")\n",
    "print(f\"  Result: {'Distributions are similar' if ks_p_value > 0.05 else 'Distributions are different'}\")\n",
    "\n",
    "# Visualization\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Histograms comparison\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.hist(original_synth_flat, bins=50, alpha=0.7, label='Original Synthetic', color='orange', density=True)\n",
    "plt.hist(generated_flat, bins=50, alpha=0.7, label='Generated', color='purple', density=True)\n",
    "plt.title('Distribution Comparison')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plots\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.boxplot([original_synth_flat, generated_flat], labels=['Original Synthetic', 'Generated'])\n",
    "plt.title('Box Plot Comparison')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Q-Q plot (Original)\n",
    "plt.subplot(3, 3, 3)\n",
    "stats.probplot(original_synth_flat, dist=\"norm\", plot=plt)\n",
    "plt.title('Original Synthetic Q-Q Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Q-Q plot (Generated)\n",
    "plt.subplot(3, 3, 4)\n",
    "stats.probplot(generated_flat, dist=\"norm\", plot=plt)\n",
    "plt.title('Generated Q-Q Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Empirical CDF\n",
    "plt.subplot(3, 3, 5)\n",
    "original_sorted = np.sort(original_synth_flat)\n",
    "generated_sorted = np.sort(generated_flat)\n",
    "original_cdf = np.arange(1, len(original_sorted) + 1) / len(original_sorted)\n",
    "generated_cdf = np.arange(1, len(generated_sorted) + 1) / len(generated_sorted)\n",
    "\n",
    "plt.plot(original_sorted, original_cdf, label='Original Synthetic', color='orange', alpha=0.8)\n",
    "plt.plot(generated_sorted, generated_cdf, label='Generated', color='purple', alpha=0.8)\n",
    "plt.title('Empirical CDF Comparison')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Kernel density estimation\n",
    "plt.subplot(3, 3, 6)\n",
    "sns.kdeplot(data=original_synth_flat, label='Original Synthetic', color='orange', alpha=0.7)\n",
    "sns.kdeplot(data=generated_flat, label='Generated', color='purple', alpha=0.7)\n",
    "plt.title('Kernel Density Estimation')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Percentile comparison\n",
    "plt.subplot(3, 3, 7)\n",
    "percentiles = np.arange(1, 100, 1)\n",
    "original_percentiles = np.percentile(original_synth_flat, percentiles)\n",
    "generated_percentiles = np.percentile(generated_flat, percentiles)\n",
    "\n",
    "plt.plot(percentiles, original_percentiles, label='Original Synthetic', color='orange', marker='o', markersize=2)\n",
    "plt.plot(percentiles, generated_percentiles, label='Generated', color='purple', marker='s', markersize=2)\n",
    "plt.title('Percentile Comparison')\n",
    "plt.xlabel('Percentile')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Scatter plot of percentiles\n",
    "plt.subplot(3, 3, 8)\n",
    "plt.scatter(original_percentiles, generated_percentiles, alpha=0.6, color='green')\n",
    "plt.plot([original_percentiles.min(), original_percentiles.max()], \n",
    "         [original_percentiles.min(), original_percentiles.max()], \n",
    "         'r--', label='Perfect Match')\n",
    "plt.title('Percentile Scatter Plot')\n",
    "plt.xlabel('Original Synthetic Percentiles')\n",
    "plt.ylabel('Generated Percentiles')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Distribution moments\n",
    "plt.subplot(3, 3, 9)\n",
    "moments = []\n",
    "for i in range(2, 5):\n",
    "    orig_moment = stats.moment(original_synth_flat, moment=i)\n",
    "    gen_moment = stats.moment(generated_flat, moment=i)\n",
    "    moments.append((orig_moment, gen_moment))\n",
    "\n",
    "x = np.arange(2, 5)\n",
    "orig_vals = [m[0] for m in moments]\n",
    "gen_vals = [m[1] for m in moments]\n",
    "\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, orig_vals, width, label='Original Synthetic', color='orange')\n",
    "plt.bar(x + width/2, gen_vals, width, label='Generated', color='purple')\n",
    "plt.xlabel('Moment Order')\n",
    "plt.ylabel('Moment Value')\n",
    "plt.title('Distribution Moments Comparison')\n",
    "plt.xticks(x)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Quantile comparison\n",
    "print(f\"\\n=== KEY QUANTILES ===\")\n",
    "quantiles = [0.05, 0.25, 0.50, 0.75, 0.95]\n",
    "for q in quantiles:\n",
    "    orig_q = np.quantile(original_synth_flat, q)\n",
    "    gen_q = np.quantile(generated_flat, q)\n",
    "    print(f\"  Q{q*100:02.0f}: Original={orig_q:.4f}, Generated={gen_q:.4f}, Diff={abs(orig_q-gen_q):.4f}\")\n",
    "\n",
    "print(f\"\\n*** INTERPRETATION ***\")\n",
    "print(f\"This analysis compares the LEARNED distribution (generated) with the INPUT distribution (original synthetic).\")\n",
    "print(f\"Good match indicates the diffusion model successfully learned the data distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7e842",
   "metadata": {},
   "source": [
    "## Key Observation: Mode Collapse Despite Good Autocorrelation\n",
    "\n",
    "**Important Finding**: The synthetic cosine experiment shows:\n",
    "- ✅ **Excellent autocorrelation matching** - Model learns temporal patterns perfectly\n",
    "- ❌ **Severe distribution collapse** - Generated data has very narrow tails, concentrated near 0\n",
    "\n",
    "**This reveals the true problem**: The mode collapse is NOT caused by DC component in frequency domain (cosines have varying means), but by the **diffusion model's tendency to generate safe, low-variance samples** during the reverse process.\n",
    "\n",
    "**Why detrending may still help**:\n",
    "- Removes per-series mean bias before training\n",
    "- Forces model to learn variance/patterns independent of absolute level\n",
    "- Restores realistic means during generation\n",
    "- May prevent model from \"playing it safe\" by regressing to global mean\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
